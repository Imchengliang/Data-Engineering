{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061744eb-18fc-448e-b09d-d5e352e3e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SQLContext\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel,LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from vaderSentiment import vaderSentiment\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ffc50c9-25a8-4e6a-9b80-debb2c557554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/22 21:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# New API\n",
    "a = 8\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"test4\")\\\n",
    "        .master(\"spark://192.168.2.147:7077\")\\\n",
    "        .config(\"spark.cores.max\", a)\\\n",
    "        .config(\"spark.executor.memory\", \"6G\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")\\\n",
    "        .config(\"spark.driver.port\", 9998)\\\n",
    "        .config(\"spark.blockManager.port\", 10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")\n",
    "sql_context = SQLContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1faaed-48d1-47cb-ae90-c9ebfc160049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load test data set:  83.18953895568848\n"
     ]
    }
   ],
   "source": [
    "# read the json file into dataframe\n",
    "t = time.time()\n",
    "t0 = time.time()\n",
    "test_data = sql_context.read.json(\"hdfs://192.168.2.147:9000/test1/RC_2012-04\")\n",
    "print(\"Time to load test data set: \", time.time()-t0)\n",
    "train_data = sql_context.read.csv(\"hdfs://192.168.2.147:9000/test1/training.1600000.processed.noemoticon.csv\")\n",
    "#train_data.printSchema()\n",
    "#test_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a210da8-ce76-4f63-ae06-af11ea0f2214",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0797a8-613b-4ba3-ac3b-57a5cfd68391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for data preprocess: 0.19132685661315918\n",
      "+--------------------+--------------------+-----+\n",
      "|                 _c5|   remove_stops_text|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|@switchfoot http:...|awww bummer shoul...|  0.0|\n",
      "|is upset that he ...|upset update face...|  0.0|\n",
      "|@Kenichan I dived...|dived many times ...|  0.0|\n",
      "|my whole body fee...|whole body feels ...|  0.0|\n",
      "|@nationwideclass ...|  behaving m mad see|  0.0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                body|   remove_stops_text|\n",
      "+--------------------+--------------------+\n",
      "|I think an equall...|think equally rea...|\n",
      "|[Hi.](http://meme...|hi yo dawg xzibit...|\n",
      "|yeah,im going to ...|yeah im going kee...|\n",
      "|[Here you go] (ht...|                  go|\n",
      "|[Is this better?]...| better com dm m png|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuations mentions and alphanumeric characters\n",
    "def remove_features(data):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data = data.lower()\n",
    "    # remove hyperlinks\n",
    "    data = url_re.sub(' ', data)\n",
    "    # remove @mentions\n",
    "    data = mention_re.sub(' ', data)\n",
    "    # remove puncuation\n",
    "    data = punc_re.sub(' ', data)\n",
    "    # remove numeric 'words'\n",
    "    data = num_re.sub(' ', data)\n",
    "    # remove non a-z 0-9 characters and words shorter than 1 characters\n",
    "    list_pos = 0\n",
    "    cleaned = ''\n",
    "    for word in data.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word):\n",
    "                cleaned = word\n",
    "            else:\n",
    "                cleaned = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word):\n",
    "                cleaned = cleaned + ' ' + word\n",
    "            else:\n",
    "                cleaned += ' '\n",
    "        list_pos += 1\n",
    "    # remove unwanted space, *.split() will automatically split on\n",
    "    # whitespace and discard duplicates, the \" \".join() joins the\n",
    "    # resulting list into one string.\n",
    "    return \" \".join(cleaned.split())\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stops(data):\n",
    "    # expects a string\n",
    "    # stops = set(stopwords.words(\"english\"))\n",
    "    stopwords = [u'rt', u're', u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your',\n",
    "             u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers',\n",
    "             u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what',\n",
    "             u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were',\n",
    "             u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a',\n",
    "             u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by',\n",
    "             u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after',\n",
    "             u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under',\n",
    "             u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all',\n",
    "             u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not',\n",
    "             u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don',\n",
    "             u'should', u'now']\n",
    "    list_pos = 0\n",
    "    cleaned = ''\n",
    "    text = data.split()\n",
    "    for word in text:\n",
    "        if word not in stopwords:\n",
    "            # rebuild cleaned\n",
    "            if list_pos == 0:\n",
    "                cleaned = word\n",
    "            else:\n",
    "                cleaned = cleaned + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned\n",
    "\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "\n",
    "# Preprocess train data\n",
    "t1 = time.time()\n",
    "new_train_data = train_data.withColumn('remove_features_text', remove_features_udf(train_data['_c5']))\n",
    "new_train_data = new_train_data.withColumn('remove_stops_text', remove_stops_udf('remove_features_text'))\n",
    "new_train_data = new_train_data.withColumn('label', new_train_data['_c0'].cast(DoubleType()))\n",
    "\n",
    "# Preprocess test data\n",
    "new_test_data = test_data[test_data.body!='[deleted]']\n",
    "new_test_data = new_test_data.withColumn('remove_features_text', remove_features_udf(new_test_data['body']))\n",
    "new_test_data = new_test_data.withColumn('remove_stops_text', remove_stops_udf('remove_features_text'))\n",
    "new_test_data = new_test_data.select('body', 'remove_stops_text')\n",
    "print('time for data preprocess:', time.time()-t1)\n",
    "new_train_data.select('_c5', 'remove_stops_text', 'label').show(5)\n",
    "new_test_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd6a9e-6ce5-4946-ba2c-d88596163d8f",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4daaa28-c07b-4760-9cee-6f6d03bbbfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for model training: 122.59346675872803\n",
      "time for testing: 0.17111730575561523\n",
      "+--------------------+----------+\n",
      "|                body|prediction|\n",
      "+--------------------+----------+\n",
      "|I think an equall...|       4.0|\n",
      "|[Hi.](http://meme...|       4.0|\n",
      "|yeah,im going to ...|       4.0|\n",
      "|[Here you go] (ht...|       4.0|\n",
      "|[Is this better?]...|       4.0|\n",
      "|Thanks! I tweaked...|       4.0|\n",
      "|Noooooooooooooooo...|       4.0|\n",
      "|Once in a while I...|       0.0|\n",
      "|Mudkipz. I liek t...|       4.0|\n",
      "|You know the Arby...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the document\n",
    "tokenizer = Tokenizer(inputCol='remove_stops_text', outputCol='words')\n",
    "wordsDataFrame = tokenizer.transform(new_train_data)\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='hashing')\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol='features')\n",
    "\n",
    "t2 = time.time()\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "# Train the model\n",
    "model = pipeline.fit(new_train_data)\n",
    "print('time for model training:', time.time()-t2)\n",
    "\n",
    "t3 = time.time()\n",
    "# Predict sentiment\n",
    "prediction = model.transform(new_test_data)\n",
    "print('time for testing:', time.time()-t3)\n",
    "prediction.select('body', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c47c15-0322-47c0-ba99-a6b1b744c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17559861"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for output JSON file\n",
    "output_df = prediction.select('body', 'prediction')\n",
    "output_df.printSchema()\n",
    "output_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633927d-2cd9-4ed4-8ab3-86453c1c3066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:======================================================> (80 + 2) / 82]\r"
     ]
    }
   ],
   "source": [
    "# output_df.write.json(\"test1001.json\")\n",
    "output_df.repartition(1).write.json(\"output3/test1204\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ca339-ffef-45a1-8056-866e38e360b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                         (0 + 0) / 13]\r"
     ]
    }
   ],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64565fae-8514-4b34-8bbd-4dae09a73a7f",
   "metadata": {},
   "source": [
    "**Import data into MongoDB database**\n",
    "\n",
    "mongoimport --db=reddit --collection=result200512 --file=test0512.json<br>\n",
    "mongoimport --db=reddit --collection=result201001 --file=test1001.json<br>\n",
    "mongoimport --db=reddit --collection=result201011 --file=test1011.json<br>\n",
    "mongoimport --db=reddit --collection=result201204 --file=test1204.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985ce07-f6bd-4e30-9db6-a02c2a8448b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
